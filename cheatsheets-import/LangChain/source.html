<!DOCTYPE html>
<html lang="en" >

<head>
  <meta charset="UTF-8">
  

    <link rel="apple-touch-icon" type="image/png" href="https://cpwebassets.codepen.io/assets/favicon/apple-touch-icon-5ae1a0698dcc2402e9712f7d01ed509a57814f994c660df9f7a952f3060705ee.png" />

    <meta name="apple-mobile-web-app-title" content="CodePen">

    <link rel="icon" type="image/x-icon" href="https://cpwebassets.codepen.io/assets/favicon/favicon-aec34940fbc1a6e787974dcd360f2c6b63348d4b1f4e06c77743096d55480f33.ico" />

    <link rel="mask-icon" type="image/x-icon" href="https://cpwebassets.codepen.io/assets/favicon/logo-pin-b4b4269c16397ad2f0f7a01bcdf513a1994f4c94b8af2f191c09eb0d601762b1.svg" color="#111" />



  
  

  <title>LangChain Python å¼€å‘é€ŸæŸ¥è¡¨</title>

    <link rel="canonical" href="https://codepen.io/ccwq/pen/zxrXmgE">
  
  
  
  

  <script>
  window.console = window.console || function(t) {};
</script>

  
  
</head>

<body translate="no">
  <!doctype html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<title>LangChain Python å¼€å‘é€ŸæŸ¥è¡¨</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
html,body{height:100%;margin:0;padding:0;background:linear-gradient(130deg,#303d54 0%,#23304a 48%,#171b27 100%);font-family:'Segoe UI','PingFang SC','Microsoft YaHei',Arial,sans-serif;font-size:14px;line-height:1.2;color:#e7e7ea;overflow-x:hidden}body{min-height:100vh;padding:0}.container{width:100%;max-width:100%;margin:0 auto;padding:8px 4px 16px 4px;box-sizing:border-box}.cheat-columns{column-count:4;column-gap:16px;transition:column-width 0.2s;margin-top:12px}@media (max-width:1500px){.cheat-columns{column-count:3}}@media (max-width:1100px){.cheat-columns{column-count:2;column-gap:12px}}@media (max-width:750px){.cheat-columns{column-count:1}}.card{background:rgba(29,36,53,0.85);border-radius:12px;box-shadow:0 2px 12px 1px rgba(0,0,0,0.18);backdrop-filter:blur(6px);margin-bottom:14px;padding:14px 14px 8px 14px;break-inside:avoid;border:1px solid rgba(78,85,130,.12);min-width:0;box-sizing:border-box}.card h2{margin:-6px 0 6px 0;font-size:16px;font-weight:600;color:#fdbe3f;letter-spacing:0.6px}.card h3{font-size:14px;font-weight:bold;color:#93cdfc;margin:8px 0 6px 0}.card ul,.card ol{padding-left:16px;margin:6px 0 8px 0}.card li{margin-bottom:4px;word-break:break-all;font-size:14px;line-height:1.2}code,pre{font-family:'Fira Mono','Consolas','Menlo',monospace;font-size:13px;background:#181d26;color:#e2ffd0;border-radius:5px;line-height:1.2;padding:0 2px;letter-spacing:0.1px;word-wrap:break-word;word-break:break-all}pre{background:#1a1e2b;border-radius:6px;padding:8px 10px;margin:6px 0 8px 0;color:#e6e8ef;overflow-x:auto;font-size:13px;line-height:1.2;box-shadow:0 1px 5px 0 rgba(0,12,22,.15)}@media (max-width:750px){pre{font-size:12px;padding:6px 8px}}.desc{color:#acc3f8;font-style:italic;margin:4px 0 6px 0;font-size:14px;line-height:1.2}.panel{display:flex;align-items:center;background:rgba(36,41,59,0.91);border-radius:12px;box-shadow:0 1px 6px 0 rgba(0,0,0,0.1);padding:8px 16px;margin-bottom:12px;border:1px solid #353e63;gap:16px;flex-wrap:wrap;box-sizing:border-box}.panel label{font-size:13px;color:#ffe49b;font-weight:bold;margin-right:5px;letter-spacing:0.2px}.slider-bar{min-width:120px;margin-right:6px}.slider-val{color:#ffce5b;font-size:13px;margin-left:4px;min-width:28px;display:inline-block;font-weight:600}@media (max-width:660px){.panel{flex-direction:column;gap:8px;padding:6px 4px}}
</style>
</head>
<body>
<div class="container">
<div class="panel">
<span style="color:#7bfbb7;font-size:16px;margin-right:auto;">ğŸ¦œğŸ”— LangChain Python å¼€å‘é€ŸæŸ¥è¡¨</span>
<label for="columnWidth">ğŸ”§ è°ƒæ•´æ¯åˆ—å®½åº¦ï¼š</label>
<input id="columnWidth" class="slider-bar" type="range" min="300" max="660" value="340">
<span id="widthVal" class="slider-val">340px</span>
</div>
<div class="cheat-columns" id="columns">

<!-- å®‰è£…ä¸åŸºç¡€ -->
<div class="card">
<h2>ğŸ“¦ å®‰è£…ä¸é…ç½®</h2>
<h3>å®‰è£…æ ¸å¿ƒåº“</h3>
<pre>pip install langchain
pip install langchain-core
pip install langchain-community</pre>
<h3>å®‰è£…ç‰¹å®šé›†æˆ</h3>
<pre># OpenAI
pip install langchain-openai

# Anthropic
pip install langchain-anthropic

# Google
pip install langchain-google-vertexai

# å‘é‡å­˜å‚¨
pip install chromadb faiss-cpu</pre>
<div class="desc">LangChain v1.0 å·²å‘å¸ƒï¼Œå»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ä»¥è·å¾—æœ€ä½³æ€§èƒ½</div>
</div>

<!-- æ¨¡å‹é›†æˆ -->
<div class="card">
<h2>ğŸ¤– æ¨¡å‹é›†æˆ (Models)</h2>
<h3>åˆå§‹åŒ– LLM</h3>
<pre>from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# OpenAI
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.7,
    api_key="your-key"
)

# Anthropic Claude
llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    temperature=0.7
)</pre>
<h3>è°ƒç”¨æ¨¡å‹</h3>
<pre># åŒæ­¥è°ƒç”¨
response = llm.invoke("ä½ å¥½")

# å¼‚æ­¥è°ƒç”¨
response = await llm.ainvoke("ä½ å¥½")

# æµå¼è¾“å‡º
for chunk in llm.stream("è®²ä¸ªæ•…äº‹"):
    print(chunk.content, end="")</pre>
<div class="desc">æ”¯æŒ OpenAIã€Anthropicã€Googleã€Cohere ç­‰ä¸»æµ LLM æä¾›å•†</div>
</div>

<!-- Prompt æ¨¡æ¿ -->
<div class="card">
<h2>ğŸ“ Prompt æ¨¡æ¿ (Prompts)</h2>
<h3>åŸºç¡€ Prompt æ¨¡æ¿</h3>
<pre>from langchain_core.prompts import PromptTemplate

# åˆ›å»ºæ¨¡æ¿
template = """æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}
é—®é¢˜ï¼š{question}
å›ç­”ï¼š"""

prompt = PromptTemplate.from_template(template)

# æ ¼å¼åŒ–
result = prompt.format(
    context="LangChain æ˜¯ä¸€ä¸ªæ¡†æ¶",
    question="ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"
)</pre>
<h3>èŠå¤©æ¶ˆæ¯æ¨¡æ¿</h3>
<pre>from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"),
    ("user", "{input}")
])

messages = prompt.format_messages(
    input="ä½ å¥½"
)</pre>
<div class="desc">æ¨¡æ¿æ”¯æŒ f-string å’Œ Jinja2 æ ¼å¼ï¼Œæ¨èä½¿ç”¨ f-string</div>
</div>

<!-- LCEL é“¾å¼è°ƒç”¨ -->
<div class="card">
<h2>ğŸ”— LCEL é“¾å¼è°ƒç”¨</h2>
<h3>åŸºç¡€é“¾</h3>
<pre>from langchain_core.output_parsers import StrOutputParser

# ä½¿ç”¨ç®¡é“è¿ç®—ç¬¦ |
chain = prompt | llm | StrOutputParser()

# æ‰§è¡Œé“¾
result = chain.invoke({
    "context": "ä¸Šä¸‹æ–‡",
    "question": "é—®é¢˜"
})</pre>
<h3>å¹¶è¡Œæ‰§è¡Œ</h3>
<pre>from langchain_core.runnables import RunnableParallel

map_chain = RunnableParallel({
    "code": code_chain,
    "tests": test_chain
})

result = map_chain.invoke({
    "requirement": "ç”Ÿæˆæ–æ³¢é‚£å¥‘æ•°åˆ—"
})</pre>
<h3>é“¾å¼æ–¹æ³•</h3>
<pre># åŒæ­¥æ‰§è¡Œ
chain.invoke(input)

# å¼‚æ­¥æ‰§è¡Œ
await chain.ainvoke(input)

# æ‰¹é‡å¤„ç†
chain.batch([input1, input2])

# æµå¼è¾“å‡º
for chunk in chain.stream(input):
    print(chunk)</pre>
<div class="desc">LCEL æä¾›ç»Ÿä¸€æ¥å£ã€è‡ªåŠ¨å¹¶è¡ŒåŒ–å’Œå®Œæ•´çš„æµå¼æ”¯æŒ</div>
</div>

<!-- Agents ä»£ç† -->
<div class="card">
<h2>ğŸ¯ Agents ä»£ç†</h2>
<h3>åˆ›å»ºç®€å•ä»£ç†</h3>
<pre>from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”"""
    return f"{city}å¤©æ°”æ™´æœ—"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
    system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
)

response = agent.invoke({
    "messages": [
        {"role": "user", "content": "åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ"}
    ]
})</pre>
<h3>ä½¿ç”¨å·¥å…·è£…é¥°å™¨</h3>
<pre>from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
    """å°†ä¸¤ä¸ªæ•´æ•°ç›¸ä¹˜"""
    return a * b

@tool
def add(a: int, b: int) -> int:
    """å°†ä¸¤ä¸ªæ•´æ•°ç›¸åŠ """
    return a + b

tools = [multiply, add]</pre>
<div class="desc">ä»£ç†å¯ä»¥åŠ¨æ€å†³å®šä½•æ—¶ä½¿ç”¨å·¥å…·å¹¶æ‰§è¡Œå¤šæ­¥æ¨ç†</div>
</div>

<!-- Tools å·¥å…· -->
<div class="card">
<h2>ğŸ› ï¸ Tools å·¥å…·</h2>
<h3>å®šä¹‰å·¥å…·</h3>
<pre>from langchain.tools import Tool

def search_tool(query: str) -> str:
    """æœç´¢ä¿¡æ¯"""
    return f"æœç´¢ç»“æœï¼š{query}"

tool = Tool(
    name="Search",
    func=search_tool,
    description="ç”¨äºæœç´¢ä¿¡æ¯çš„å·¥å…·"
)</pre>
<h3>ç»‘å®šå·¥å…·åˆ°æ¨¡å‹</h3>
<pre>from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools([multiply, add])

result = llm_with_tools.invoke(
    "5 ä¹˜ä»¥ 3 æ˜¯å¤šå°‘ï¼Ÿ"
)</pre>
<h3>å·¥å…·æ‰§è¡Œä»£ç†</h3>
<pre>from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain import hub

prompt = hub.pull("hwchase17/openai-tools-agent")

agent = create_tool_calling_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True
)</pre>
<div class="desc">å·¥å…·è®© LLM å¯ä»¥æ‰§è¡Œå¤–éƒ¨æ“ä½œå¦‚æœç´¢ã€è®¡ç®—ã€æ•°æ®åº“æŸ¥è¯¢ç­‰</div>
</div>

<!-- æ–‡æ¡£åŠ è½½å™¨ -->
<div class="card">
<h2>ğŸ“„ æ–‡æ¡£åŠ è½½å™¨ (Document Loaders)</h2>
<h3>æ–‡æœ¬æ–‡ä»¶</h3>
<pre>from langchain_community.document_loaders import TextLoader

loader = TextLoader("document.txt")
documents = loader.load()</pre>
<h3>PDF æ–‡ä»¶</h3>
<pre>from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("document.pdf")
pages = loader.load_and_split()</pre>
<h3>ç½‘é¡µå†…å®¹</h3>
<pre>from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://example.com")
docs = loader.load()</pre>
<h3>CSV æ–‡ä»¶</h3>
<pre>from langchain_community.document_loaders import CSVLoader

loader = CSVLoader("data.csv")
data = loader.load()</pre>
<div class="desc">æ”¯æŒå¤šç§æ ¼å¼ï¼šæ–‡æœ¬ã€PDFã€Wordã€CSVã€JSONã€ç½‘é¡µç­‰</div>
</div>

<!-- æ–‡æœ¬åˆ†å‰²å™¨ -->
<div class="card">
<h2>âœ‚ï¸ æ–‡æœ¬åˆ†å‰²å™¨ (Text Splitters)</h2>
<h3>å­—ç¬¦åˆ†å‰²</h3>
<pre>from langchain_text_splitters import CharacterTextSplitter

splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separator="\n"
)

chunks = splitter.split_documents(documents)</pre>
<h3>é€’å½’å­—ç¬¦åˆ†å‰²</h3>
<pre>from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", " ", ""]
)

texts = splitter.split_documents(docs)</pre>
<h3>ä»£ç åˆ†å‰²å™¨</h3>
<pre>from langchain_text_splitters import PythonCodeTextSplitter

splitter = PythonCodeTextSplitter(
    chunk_size=500,
    chunk_overlap=0
)

code_chunks = splitter.split_text(code_text)</pre>
<div class="desc">åˆç†çš„åˆ†å—å¤§å°å’Œé‡å åº¦èƒ½æé«˜æ£€ç´¢è´¨é‡</div>
</div>

<!-- Embeddings åµŒå…¥ -->
<div class="card">
<h2>ğŸ§¬ Embeddings åµŒå…¥æ¨¡å‹</h2>
<h3>OpenAI Embeddings</h3>
<pre>from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large"
)

# åµŒå…¥æŸ¥è¯¢
query_vector = embeddings.embed_query("æŸ¥è¯¢æ–‡æœ¬")

# æ‰¹é‡åµŒå…¥æ–‡æ¡£
doc_vectors = embeddings.embed_documents([
    "æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"
])</pre>
<h3>HuggingFace Embeddings</h3>
<pre>from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)</pre>
<div class="desc">å‘é‡ç»´åº¦ï¼šOpenAI 3072ç»´ï¼ŒBGE 1024ç»´ï¼Œéœ€æ ¹æ®åœºæ™¯é€‰æ‹©</div>
</div>

<!-- Vector Stores å‘é‡å­˜å‚¨ -->
<div class="card">
<h2>ğŸ’¾ Vector Stores å‘é‡å­˜å‚¨</h2>
<h3>FAISS (æœ¬åœ°)</h3>
<pre>from langchain_community.vectorstores import FAISS

# åˆ›å»ºå‘é‡åº“
vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)

# ç›¸ä¼¼åº¦æœç´¢
docs = vectorstore.similarity_search(
    query="æŸ¥è¯¢å†…å®¹",
    k=4
)

# ä¿å­˜å’ŒåŠ è½½
vectorstore.save_local("faiss_index")
new_db = FAISS.load_local("faiss_index", embeddings)</pre>
<h3>Chroma (æŒä¹…åŒ–)</h3>
<pre>from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# æ·»åŠ æ–‡æ¡£
vectorstore.add_documents(new_docs)</pre>
<h3>å†…å­˜å‘é‡å­˜å‚¨</h3>
<pre>from langchain_core.vectorstores import InMemoryVectorStore

vectorstore = InMemoryVectorStore(embeddings)
vectorstore.add_documents(documents)</pre>
<div class="desc">é€‰æ‹©å‘é‡åº“ï¼šæœ¬åœ°å¼€å‘ç”¨ FAISSï¼Œç”Ÿäº§ç¯å¢ƒè€ƒè™‘ Pineconeã€Weaviate</div>
</div>

<!-- Retrievers æ£€ç´¢å™¨ -->
<div class="card">
<h2>ğŸ” Retrievers æ£€ç´¢å™¨</h2>
<h3>å‘é‡å­˜å‚¨æ£€ç´¢å™¨</h3>
<pre># åŸºç¡€æ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

docs = retriever.invoke("æŸ¥è¯¢é—®é¢˜")</pre>
<h3>MMR æ£€ç´¢</h3>
<pre># æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ£€ç´¢ï¼ˆæé«˜å¤šæ ·æ€§ï¼‰
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 4,
        "fetch_k": 20,
        "lambda_mult": 0.5
    }
)</pre>
<h3>é˜ˆå€¼æ£€ç´¢</h3>
<pre># åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,
        "k": 5
    }
)</pre>
<div class="desc">æ£€ç´¢å™¨æ˜¯è¿æ¥å‘é‡å­˜å‚¨å’Œåº”ç”¨çš„æ¡¥æ¢</div>
</div>

<!-- RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ -->
<div class="card">
<h2>ğŸ“ RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ</h2>
<h3>åŸºç¡€ RAG é“¾</h3>
<pre>from langchain_core.runnables import RunnablePassthrough

template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, 
     "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

answer = rag_chain.invoke("ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ")</pre>
<h3>å¸¦æ¥æºçš„ RAG</h3>
<pre>from langchain.chains import RetrievalQAWithSourcesChain

chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

result = chain.invoke({"question": "é—®é¢˜å†…å®¹"})</pre>
<div class="desc">RAG å°†å¤–éƒ¨çŸ¥è¯†æ³¨å…¥ LLMï¼Œè§£å†³å¹»è§‰å’ŒçŸ¥è¯†è¿‡æ—¶é—®é¢˜</div>
</div>

<!-- Output Parsers è¾“å‡ºè§£æå™¨ -->
<div class="card">
<h2>ğŸ“¤ Output Parsers è¾“å‡ºè§£æå™¨</h2>
<h3>å­—ç¬¦ä¸²è§£æå™¨</h3>
<pre>from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()
chain = prompt | llm | parser</pre>
<h3>JSON è§£æå™¨</h3>
<pre>from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class Person(BaseModel):
    name: str = Field(description="å§“å")
    age: int = Field(description="å¹´é¾„")

parser = JsonOutputParser(pydantic_object=Person)

prompt = PromptTemplate(
    template="æå–ä¿¡æ¯ï¼š{query}\n{format_instructions}",
    input_variables=["query"],
    partial_variables={
        "format_instructions": parser.get_format_instructions()
    }
)

chain = prompt | llm | parser</pre>
<h3>åˆ—è¡¨è§£æå™¨</h3>
<pre>from langchain_core.output_parsers import CommaSeparatedListOutputParser

parser = CommaSeparatedListOutputParser()
chain = prompt | llm | parser
# è¾“å‡ºï¼š['é¡¹ç›®1', 'é¡¹ç›®2', 'é¡¹ç›®3']</pre>
<div class="desc">è¾“å‡ºè§£æå™¨å°† LLM æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®</div>
</div>

<!-- Memory è®°å¿† -->
<div class="card">
<h2>ğŸ§  Memory è®°å¿†ç®¡ç†</h2>
<h3>å¯¹è¯ç¼“å†²è®°å¿†</h3>
<pre>from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="chat_history"
)

# ä¿å­˜ä¸Šä¸‹æ–‡
memory.save_context(
    {"input": "ä½ å¥½"},
    {"output": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„ï¼Ÿ"}
)

# åŠ è½½è®°å¿†
history = memory.load_memory_variables({})</pre>
<h3>å¯¹è¯æ‘˜è¦è®°å¿†</h3>
<pre>from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(
    llm=llm,
    return_messages=True
)</pre>
<h3>å¸¦è®°å¿†çš„é“¾</h3>
<pre>from langchain_core.runnables import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

chain = RunnableWithMessageHistory(
    prompt | llm | StrOutputParser(),
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)</pre>
<div class="desc">è®°å¿†è®©å¯¹è¯å…·æœ‰ä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼Œæ”¯æŒå¤šç§å­˜å‚¨ç­–ç•¥</div>
</div>

<!-- Callbacks å›è°ƒ -->
<div class="card">
<h2>ğŸ“ Callbacks å›è°ƒæœºåˆ¶</h2>
<h3>è‡ªå®šä¹‰å›è°ƒ</h3>
<pre>from langchain_core.callbacks import BaseCallbackHandler

class MyCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print("LLM å¼€å§‹è¿è¡Œ")
    
    def on_llm_end(self, response, **kwargs):
        print("LLM è¿è¡Œç»“æŸ")
    
    def on_llm_new_token(self, token, **kwargs):
        print(f"æ–° token: {token}")

# ä½¿ç”¨å›è°ƒ
chain.invoke(
    input_data,
    config={"callbacks": [MyCallbackHandler()]}
)</pre>
<h3>æµå¼å›è°ƒ</h3>
<pre>class StreamingHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        print(token, end="", flush=True)

llm = ChatOpenAI(
    streaming=True,
    callbacks=[StreamingHandler()]
)</pre>
<div class="desc">å›è°ƒç”¨äºç›‘æ§ã€æ—¥å¿—è®°å½•ã€æµå¼è¾“å‡ºå’Œè‡ªå®šä¹‰è¡Œä¸º</div>
</div>

<!-- LangSmith ç›‘æ§ -->
<div class="card">
<h2>ğŸ“Š LangSmith è°ƒè¯•ä¸ç›‘æ§</h2>
<h3>å¯ç”¨è¿½è¸ª</h3>
<pre>import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "project-name"</pre>
<h3>æ‰‹åŠ¨è¿½è¸ª</h3>
<pre>from langsmith import traceable

@traceable
def my_function(input_text):
    # è‡ªåŠ¨è¿½è¸ªæ­¤å‡½æ•°
    return llm.invoke(input_text)</pre>
<div class="desc">LangSmith æä¾›å¯è§†åŒ–è¿½è¸ªã€æ€§èƒ½åˆ†æå’Œè°ƒè¯•å·¥å…·</div>
</div>

<!-- Streaming æµå¼å¤„ç† -->
<div class="card">
<h2>ğŸŒŠ Streaming æµå¼å¤„ç†</h2>
<h3>åŸºç¡€æµå¼</h3>
<pre># æµå¼è¾“å‡º
for chunk in chain.stream(input_data):
    print(chunk, end="", flush=True)</pre>
<h3>å¼‚æ­¥æµå¼</h3>
<pre>async for chunk in chain.astream(input_data):
    print(chunk, end="", flush=True)</pre>
<h3>äº‹ä»¶æµ</h3>
<pre>async for event in chain.astream_events(
    input_data,
    version="v2"
):
    if event["event"] == "on_llm_new_token":
        print(event["data"]["chunk"])</pre>
<div class="desc">æµå¼è¾“å‡ºé™ä½é¦–å­—èŠ‚æ—¶é—´ï¼Œæå‡ç”¨æˆ·ä½“éªŒ</div>
</div>

<!-- é”™è¯¯å¤„ç†ä¸é‡è¯• -->
<div class="card">
<h2>âš ï¸ é”™è¯¯å¤„ç†ä¸é‡è¯•</h2>
<h3>æ·»åŠ åå¤‡æ–¹æ¡ˆ</h3>
<pre># ä¸»é“¾å¤±è´¥æ—¶ä½¿ç”¨å¤‡ç”¨é“¾
chain_with_fallback = chain.with_fallbacks([
    fallback_chain
])

result = chain_with_fallback.invoke(input_data)</pre>
<h3>é‡è¯•é…ç½®</h3>
<pre>from langchain_core.runnables import RunnableRetry

chain_with_retry = RunnableRetry(
    bound=chain,
    max_attempt_number=3,
    wait_exponential_jitter=True
)</pre>
<h3>å¼‚å¸¸å¤„ç†</h3>
<pre>try:
    result = chain.invoke(input_data)
except Exception as e:
    print(f"é”™è¯¯ï¼š{e}")
    # å¤„ç†é”™è¯¯</pre>
<div class="desc">ç”Ÿäº§ç¯å¢ƒå¿…é¡»å¤„ç† API å¤±è´¥ã€è¶…æ—¶å’Œæ— æ•ˆè¾“å…¥</div>
</div>

<!-- å¸¸ç”¨é›†æˆ -->
<div class="card">
<h2>ğŸ”Œ å¸¸ç”¨é›†æˆå·¥å…·</h2>
<h3>æœç´¢å·¥å…·</h3>
<pre># DuckDuckGo æœç´¢
from langchain_community.tools import DuckDuckGoSearchRun

search = DuckDuckGoSearchRun()

# Wikipedia
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

wikipedia = WikipediaQueryRun(
    api_wrapper=WikipediaAPIWrapper()
)</pre>
<h3>Python REPL</h3>
<pre>from langchain_experimental.tools import PythonREPLTool

python_repl = PythonREPLTool()
result = python_repl.run("print(2 + 2)")</pre>
<h3>API è°ƒç”¨</h3>
<pre>from langchain_community.utilities import RequestsWrapper

requests = RequestsWrapper()
response = requests.get("https://api.example.com")</pre>
<div class="desc">é›†æˆå¤–éƒ¨æœåŠ¡æ‰©å±• LLM èƒ½åŠ›è¾¹ç•Œ</div>
</div>

<!-- æœ€ä½³å®è·µ -->
<div class="card">
<h2>âœ¨ æœ€ä½³å®è·µ</h2>
<ul>
<li><strong>ä½¿ç”¨ LCELï¼š</strong>ä¼˜å…ˆä½¿ç”¨ç®¡é“è¯­æ³•æ„å»ºé“¾ï¼Œä»£ç æ›´æ¸…æ™°å¯ç»´æŠ¤</li>
<li><strong>åˆç†åˆ†å—ï¼š</strong>chunk_size 500-1000 å­—ç¬¦ï¼Œoverlap 10-20%</li>
<li><strong>é€‰æ‹©æ¨¡å‹ï¼š</strong>ç®€å•ä»»åŠ¡ç”¨å°æ¨¡å‹ï¼Œå¤æ‚æ¨ç†ç”¨å¤§æ¨¡å‹</li>
<li><strong>å‘é‡å­˜å‚¨ï¼š</strong>æœ¬åœ°å¼€å‘ FAISSï¼Œç”Ÿäº§ç¯å¢ƒæ‰˜ç®¡æœåŠ¡</li>
<li><strong>æç¤ºå·¥ç¨‹ï¼š</strong>æ¸…æ™°æŒ‡ä»¤ + ç¤ºä¾‹ + è¾“å‡ºæ ¼å¼çº¦æŸ</li>
<li><strong>é”™è¯¯å¤„ç†ï¼š</strong>æ·»åŠ é‡è¯•ã€åå¤‡å’Œè¶…æ—¶æœºåˆ¶</li>
<li><strong>ç›‘æ§è¿½è¸ªï¼š</strong>ç”Ÿäº§ç¯å¢ƒå¯ç”¨ LangSmith è¿½è¸ª</li>
<li><strong>æˆæœ¬ä¼˜åŒ–ï¼š</strong>ç¼“å­˜ç»“æœã€æ‰¹é‡å¤„ç†ã€é€‰æ‹©åˆé€‚æ¨¡å‹</li>
<li><strong>å®‰å…¨è€ƒè™‘ï¼š</strong>éªŒè¯ç”¨æˆ·è¾“å…¥ã€é™åˆ¶å·¥å…·æƒé™ã€ä¿æŠ¤ API å¯†é’¥</li>
</ul>
</div>

<!-- å¸¸è§é—®é¢˜ -->
<div class="card">
<h2>â“ å¸¸è§é—®é¢˜</h2>
<ul>
<li><strong>Qï¼šå¦‚ä½•é€‰æ‹©å‘é‡å­˜å‚¨ï¼Ÿ</strong><br>Aï¼šå¼€å‘ç”¨ FAISS/Chromaï¼Œç”Ÿäº§ç”¨ Pinecone/Weaviate</li>
<li><strong>Qï¼šRAG æ£€ç´¢è´¨é‡å·®ï¼Ÿ</strong><br>Aï¼šä¼˜åŒ–åˆ†å—ç­–ç•¥ã€è°ƒæ•´ embedding æ¨¡å‹ã€ä½¿ç”¨æ··åˆæ£€ç´¢</li>
<li><strong>Qï¼šå¦‚ä½•é™ä½æˆæœ¬ï¼Ÿ</strong><br>Aï¼šä½¿ç”¨æ›´å°æ¨¡å‹ã€å®æ–½ç¼“å­˜ã€æ‰¹é‡è°ƒç”¨ã€ä¼˜åŒ– prompt é•¿åº¦</li>
<li><strong>Qï¼šå¦‚ä½•å¤„ç†é•¿æ–‡æ¡£ï¼Ÿ</strong><br>Aï¼šä½¿ç”¨ RecursiveCharacterTextSplitter + Map-Reduce é“¾</li>
<li><strong>Qï¼šAgent æ€§èƒ½æ…¢ï¼Ÿ</strong><br>Aï¼šå‡å°‘å·¥å…·æ•°é‡ã€ä¼˜åŒ–å·¥å…·æè¿°ã€è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°</li>
</ul>
</div>

<!-- èµ„æºé“¾æ¥ -->
<div class="card">
<h2>ğŸ”— å­¦ä¹ èµ„æº</h2>
<ul>
<li><strong>å®˜æ–¹æ–‡æ¡£ï¼š</strong>https://python.langchain.com</li>
<li><strong>API å‚è€ƒï¼š</strong>https://api.python.langchain.com</li>
<li><strong>LangSmithï¼š</strong>https://smith.langchain.com</li>
<li><strong>GitHubï¼š</strong>https://github.com/langchain-ai/langchain</li>
<li><strong>ç¤¾åŒºè®ºå›ï¼š</strong>https://discord.gg/langchain</li>
<li><strong>æ¨¡æ¿åº“ï¼š</strong>https://templates.langchain.com</li>
</ul>
</div>

</div>
</div>
<script>
document.getElementById('columnWidth').addEventListener('input', function(e) {
const width = e.target.value;
document.getElementById('widthVal').textContent = width + 'px';
const columns = document.getElementById('columns');
columns.style.columnWidth = width + 'px';
});
</script>
</body>
</html>
  
  
  
</body>

</html>